{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Used functions.\n"
      ],
      "metadata": {
        "id": "zUqohfRhZzJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "!pip install torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the shape of the signature image\n",
        "C, H, W = 1, 224, 224  # Example values, update as needed\n",
        "D = 64  # Dimension of each block\n",
        "k=5 # no. of nearset neighbours.\n",
        "#  we adopt AdamW optimizer with lr = 0.01\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, C, D):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=C, out_channels=D, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build_graph(features, k):\n",
        "    \"\"\"\n",
        "    Builds an undirected graph from the signature image features using the K nearest neighbors algorithm.\n",
        "\n",
        "    Args:\n",
        "        features (np.ndarray): 2D array of shape (num_vertices, feature_dim) containing the feature vectors for each vertex.\n",
        "        k (int): Number of nearest neighbors to consider for each vertex.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Edge index array of shape (2, num_edges) representing the undirected edges in the graph.\n",
        "    \"\"\"\n",
        "    num_vertices = features.shape[0]    #  ----> N\n",
        "    features= features.detach().numpy() # ----> D\n",
        "    # feature_vectors = features.view(N, D)\n",
        "\n",
        "    # Find the K nearest neighbors for each vertex\n",
        "    neigh = NearestNeighbors(n_neighbors=k+1, metric='euclidean')\n",
        "    neigh.fit(features)\n",
        "    knn_indices = neigh.kneighbors(features, return_distance=False)[:, 1:]  # Exclude self-connections\n",
        "\n",
        "    # Construct the edge index array\n",
        "    edges = []\n",
        "    for i in range(num_vertices):\n",
        "        for j in knn_indices[i]:\n",
        "            edges.append([i, j])\n",
        "            edges.append([j, i])  # Add reverse edge to make the graph undirected\n",
        "\n",
        "    edge_index = np.array(edges).T\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "class SigGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(SigGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    \"\"\"\n",
        "    Calculates the Euclidean distance between two tensors.\n",
        "\n",
        "    Args:\n",
        "        x1 (torch.Tensor): First tensor.\n",
        "        x2 (torch.Tensor): Second tensor.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Euclidean distance between the two tensors.\n",
        "    \"\"\"\n",
        "    return torch.sqrt(torch.sum((x1 - x2) ** 2, dim=-1))\n",
        "\n",
        "def margin_focal_loss(y_true, gr1, gr2, alpha=10, a=0.3, b=0.6, ma=0.3, mb=0.9):\n",
        "    \"\"\"\n",
        "    Calculates the margin-based focal loss as described in the image.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Ground-truth labels.\n",
        "        y_pred (torch.Tensor): Predicted distances between signature representations.\n",
        "        alpha (float): Scalar to adjust the loss weight.\n",
        "        a (float): Margin for avoiding overfitting during training for genuine-genuine pairs.\n",
        "        b (float): Margin for avoiding overfitting during training for genuine-forgery pairs.\n",
        "        mb (float): Margin for avoiding overfitting during training for forgery-forgery pairs.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Margin-based focal loss.\n",
        "    \"\"\"\n",
        "    d = euclidean_distance(gr1, gr2)\n",
        "    loss = y_true * torch.sigmoid(alpha * (d - a)) * torch.max(d - ma, dim=0)[0] ** 2 + \\\n",
        "       (1 - y_true) * torch.sigmoid(alpha * (b - d)) * torch.max(mb - d, dim=0)[0] ** 2\n",
        "    return loss.mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN3vzO3TtihS",
        "outputId": "b26a21f5-2277-4b88-a1de-aaefba32a06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model for 1st Image."
      ],
      "metadata": {
        "id": "qJqPyqg1aA8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the png image\n",
        "image_path = \"/content/sign1.png\" #koi bhi sign img download karke.\n",
        "pil_image = Image.open(image_path)\n",
        "\n",
        "# Define any necessary image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((H, W)),  # Resize the image to the desired dimensions (H, W)\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "])\n",
        "\n",
        "# Apply the transformations to the loaded image\n",
        "sign1 = transform(pil_image).unsqueeze(0)\n",
        "\n",
        "# Initialize the CNN model\n",
        "cnn_model = CNN(1, D)\n",
        "\n",
        "# Apply the CNN model to the signature image\n",
        "feature_vector1 = cnn_model(sign1) # dimensions (1,64,224,224)\n",
        "\n",
        "# Reshape the feature vectors to (N, D) tensor\n",
        "N = feature_vector1.size(2) * feature_vector1.size(3)\n",
        "# N=h*w.\n",
        "\n",
        "feature_vector1 = feature_vector1.view(-1, D) # now it has a size of N*D.\n",
        "# feature_vectors = feature_vectors.view(1, D, N).permute(0, 2, 1)\n",
        "# The -1 here means that the size of this dimension is inferred so that the total number of elements remains the same. Basically,\n",
        "#  it means “as many rows as necessary.”\n",
        "\n",
        "# D is the number of output channels, so the new shape is (batch_size * height * width, D) = (N, D).\n",
        "\n",
        "# Build the graph from the features\n",
        "edge_index1 = build_graph(feature_vector1, k)\n",
        "# (2, 40960) edge index shape\n",
        "\n",
        "# Create a Data object\n",
        "edge_index = torch.tensor(edge_index1, dtype=torch.long)\n",
        "\n",
        "# data = ...  PyTorch Geometric Data object with x and edge_index\n",
        "data1 = Data(x=feature_vector1, edge_index=edge_index)\n",
        "\n",
        "model = SigGCN(in_channels=64, hidden_channels=128, out_channels=256)\n",
        "\n",
        "# Pass the graph data through the model\n",
        "out1 = model(data1)\n",
        "print(out1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2f_ZQ4BaOZK",
        "outputId": "e0a0677e-0cd7-4b98-f4e5-885ae1e69763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50176, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model for 2nd Signature Image."
      ],
      "metadata": {
        "id": "KSv2dZxQafTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the png image\n",
        "image_path1 = \"/content/sign2.png\" #koi bhi sign img download karke.\n",
        "pil_image1 = Image.open(image_path1)\n",
        "\n",
        "# Define any necessary image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((H, W)),  # Resize the image to the desired dimensions (H, W)\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "])\n",
        "\n",
        "# Apply the transformations to the loaded image\n",
        "sign2 = transform(pil_image1).unsqueeze(0)\n",
        "\n",
        "cnn_model = CNN(1, D)\n",
        "\n",
        "# Apply the CNN model to the signature image\n",
        "feature_vector2 = cnn_model(sign2)\n",
        "\n",
        "# Reshape the feature vectors to (N, D) tensor\n",
        "N = feature_vector2.size(2) * feature_vector2.size(3)\n",
        "feature_vector2 = feature_vector2.view(-1, D) # now it has a size of N*D.\n",
        "\n",
        "# features= ... # 2D array of shape (num_vertices, feature_dim) containing the signature image features\n",
        "\n",
        "# Build the graph from the features\n",
        "edge_index2 = build_graph(feature_vector2, k)\n",
        "\n",
        "# Create a Data object\n",
        "edge_index = torch.tensor(edge_index1, dtype=torch.long)\n",
        "data2 = Data(x=feature_vector2, edge_index=edge_index)\n",
        "\n",
        "out2 = model(data2)  # Graph representation of the second signature\n",
        "\n",
        "print(out2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiSRxV0Taj8q",
        "outputId": "7645710e-8138-47c4-c4f9-46323ceca766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50176, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating final graph representation and calculating loss value."
      ],
      "metadata": {
        "id": "Mace-Ho0avUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(out1.shape)\n",
        "print(out2.shape)\n",
        "# torch.Size([50176, 256])\n",
        "# torch.Size([50176, 256])\n",
        "\n",
        "# # Calculate the loss\n",
        "y_true = 0 # Ground-truth label (0 for genuine-genuine pair, 1 for genuine-forgery pair)\n",
        "\n",
        "# alpha=10, a=0.3, b=0.6, ma=0.3, mb=0.9 - intial values, train them and get optimal values of parameters, using lr=0.01\n",
        "loss = margin_focal_loss(y_true, out1, out2, 10, 0.3, 0.6, 0.3, 0.9)\n",
        "\n",
        "# # Compare the distance with a threshold for verification\n",
        "distance = euclidean_distance(out1, out2)\n",
        "threshold = 0.5 # Threshold value for verification\n",
        "\n",
        "print(distance)\n",
        "# print(distance.shape)\n",
        "print(loss)\n",
        "\n",
        "print('Distance(loss) between two signatures is: ')\n",
        "distance_val= float(loss.item())\n",
        "print(distance_val)\n",
        "is_verified = (distance_val < threshold)\n",
        "print('Is Original ? ')\n",
        "print(is_verified)\n",
        "if(is_verified):\n",
        "  print(\"-->Original Pair\")\n",
        "else:\n",
        "  print(\"-->Forged Pair.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSRVqHZPa5h8",
        "outputId": "8779b291-84ce-4330-b967-60728817860f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50176, 256])\n",
            "torch.Size([50176, 256])\n",
            "tensor([23.0807, 23.2121, 23.0211,  ...,  0.9195,  0.8767,  1.3473],\n",
            "       grad_fn=<SqrtBackward0>)\n",
            "tensor(0.0421, grad_fn=<MeanBackward0>)\n",
            "Distance(loss) between two signatures is: \n",
            "0.04211956262588501\n",
            "Is Original ? \n",
            "True\n",
            "-->Original Pair\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
